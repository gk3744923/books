Certainly! Here's a detailed and engaging hackathon problem statement for a challenge based on Multimodal RAG (Retrieval-Augmented Generation) in the context of document understanding in banking or enterprise content management.


---

ğŸ† Hackathon Problem Statement

â€œMultimodal Intelligence Agent for Document Understanding in Bankingâ€

ğŸ“˜ Overview:

Banks and financial institutions process millions of documents â€” from scanned loan agreements and invoices to regulatory circulars and audit reports. These documents are often multi-modal in nature: containing text, tables, images, stamps, signatures, and charts.

Traditionally, querying and understanding such information requires human effort and domain knowledge. Your challenge is to build an AI-powered Multimodal Retrieval-Augmented Generation (RAG) system that can autonomously understand, retrieve, and answer queries over this rich document set.


---

ğŸ§  Objective:

Design an AI agent that can:

1. Ingest and process multimodal documents (PDFs with text, images, tables)


2. Extract and store information in a retrievable format using embeddings


3. Accept user queries in natural language


4. Retrieve relevant content across modalities (text, tables, visuals)


5. Use a multimodal LLM to generate a grounded and accurate response, citing sources (e.g., page or section)




---

ğŸ“‚ Example Queries the Agent Should Handle:

â€œWhat is the interest rate mentioned in the scanned loan agreement?â€

â€œFind all pages with XYZ Bankâ€™s stamp.â€

â€œWhat is the net profit reported in the chart in the annual report?â€

â€œWho signed the agreement dated March 2023?â€

â€œList all policy circulars with compliance clause references.â€



---

ğŸ§° Tools Allowed (Recommended, not mandatory):

Multimodal LLMs (GPT-4o, Gemini Pro Vision, Claude 3, LLaVA)

OCR libraries (Tesseract, Azure Form Recognizer, Amazon Textract)

Vision encoders (CLIP, BLIP-2)

Vector DBs (FAISS, ChromaDB, Pinecone)

Orchestration frameworks (LangChain, LangGraph, LlamaIndex)

Document parsing (PyMuPDF, pdfplumber, DocTR)



---

ğŸ“¦ Sample Document Types (organizers can provide or simulate):

Scanned loan agreements

Hand-signed contracts

Bank circulars with stamps

Financial statements with tables and charts

KYC forms with images/signatures



---

âœ… Deliverables:

A working prototype that can accept documents and user queries

Retrieval logic supporting both text and images

Answer generation with citation of document region/page

(Optional) UI to show document preview + answer traceability



---

ğŸ Evaluation Criteria:

Criteria	Weight

Accuracy of answers across modalities	30%
Quality of retrieval (text + image grounding)	25%
Innovation in architecture or approach	20%
User experience (CLI/UI/API)	15%
Explainability / Traceability	10%



---

ğŸ§  Bonus Challenges:

Extract charts and answer visual data questions

Use agentic behavior (e.g., reasoning over multiple documents or pages)

Support queries over a batch of documents (â€œWhich customers have signed forms with missing PAN?â€)



---

Let me know if you'd like a starter dataset, UI template, or code scaffold for this challenge!

